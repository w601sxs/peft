{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run installation of llm-harness from terminal:\n",
    "# git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
    "# cd lm-evaluation-harness\n",
    "# source activate pytorch_p310\n",
    "# pip install -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10f8889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/peft/examples/causal_language_modeling'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba996b",
   "metadata": {},
   "source": [
    "# Working - run from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigscience/bloomz-560m,parallelize=True \\\n",
    "    --tasks boolq \\\n",
    "    --batch_size 8 \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75a405",
   "metadata": {},
   "source": [
    "hf (pretrained=bigscience/bloomz-560m,parallelize=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
    "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
    "|boolq|      2|none  |     0|acc   |0.6483|±  |0.0084|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741daad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_COREG_PROMPT_TUNING_CAUSAL_LM \\\n",
    "    --tasks boolq \\\n",
    "    --batch_size 8 \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f9b10",
   "metadata": {},
   "source": [
    "hf (pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_COREG_PROMPT_TUNING_CAUSAL_LM), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
    "|Tasks|Version|Filter|n-shot|Metric|Value|   |Stderr|\n",
    "|-----|------:|------|-----:|------|----:|---|-----:|\n",
    "|boolq|      2|none  |     0|acc   |0.578|±  |0.0086|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_denoised_jan19 \\\n",
    "    --tasks boolq \\\n",
    "    --batch_size 8 \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f5957",
   "metadata": {},
   "source": [
    "hf (pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_denoised_jan19), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
    "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
    "|boolq|      2|none  |     0|acc   |0.5465|±  |0.0087|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570536a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_decorrelated_jan19 \\\n",
    "    --tasks boolq \\\n",
    "    --batch_size 8 \\\n",
    "    --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0741e",
   "metadata": {},
   "source": [
    "hf (pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_decorrelated_jan19), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
    "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
    "|boolq|      2|none  |     0|acc   |0.4972|±  |0.0087|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=bigscience/bloomz-560m,parallelize=True,peft=/home/ec2-user/SageMaker/peft/examples/causal_language_modeling/bigscience/bloomz-560m_decorrelated_jan19 \\\n",
    "    --tasks ought/raft \\\n",
    "    --batch_size 8 \\\n",
    "    --device cuda:0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
